{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc5881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "# PyTorch Forecasting imports\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, SMAPE\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "433182ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Fusion Transformer for Wind Power Forecasting\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Prediction horizons configuration\n",
    "PREDICTION_CONFIGS = {\n",
    "    '12h': {\n",
    "        'max_prediction_length': 12,\n",
    "        'max_encoder_length': 168,  # 7 days of history\n",
    "        'description': '12 hours ahead'\n",
    "    },\n",
    "    '24h': {\n",
    "        'max_prediction_length': 24,\n",
    "        'max_encoder_length': 168,  # 7 days of history  \n",
    "        'description': '24 hours ahead'\n",
    "    },\n",
    "    '48h': {\n",
    "        'max_prediction_length': 48,\n",
    "        'max_encoder_length': 168,  # 7 days of history\n",
    "        'description': '48 hours ahead'\n",
    "    },\n",
    "    '72h': {\n",
    "        'max_prediction_length': 72,\n",
    "        'max_encoder_length': 168,  # 7 days of history\n",
    "        'description': '72 hours ahead'\n",
    "    }\n",
    "}\n",
    "\n",
    "# TFT Model parameters\n",
    "TFT_PARAMS = {\n",
    "    'learning_rate': 0.03,\n",
    "    'hidden_size': 64,\n",
    "    'attention_head_size': 4,\n",
    "    'dropout': 0.1,\n",
    "    'hidden_continuous_size': 8,\n",
    "    'output_size': 7,  # 7 quantiles\n",
    "    'loss': QuantileLoss(),\n",
    "    'log_interval': 10,\n",
    "    'reduce_on_plateau_patience': 4,\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_PARAMS = {\n",
    "    'max_epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    'gradient_clip_val': 0.1,\n",
    "    'patience': 10\n",
    "}\n",
    "\n",
    "# Quantiles for prediction intervals\n",
    "QUANTILES = [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "\n",
    "print(\"Temporal Fusion Transformer for Wind Power Forecasting\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5286ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wind_data(file_path):\n",
    "    \"\"\"Load and preprocess wind power data for TFT\"\"\"\n",
    "    print(\"Loading wind power data...\")\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_excel(file_path)\n",
    "    print(f\"Raw data shape: {data.shape}\")\n",
    "    \n",
    "    # Parse timestamp\n",
    "    data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'], format='%Y%m%d %H:%M')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    data = data.sort_values('TIMESTAMP').reset_index(drop=True)\n",
    "    \n",
    "    # Add time index (required by TFT)\n",
    "    data['time_idx'] = range(len(data))\n",
    "    \n",
    "    # Add group identifier (single series)\n",
    "    data['series_id'] = 0\n",
    "    \n",
    "    # Add only basic time features\n",
    "    data['hour'] = data['TIMESTAMP'].dt.hour\n",
    "    data['day_of_week'] = data['TIMESTAMP'].dt.dayofweek\n",
    "    data['month'] = data['TIMESTAMP'].dt.month\n",
    "    data['day_of_year'] = data['TIMESTAMP'].dt.dayofyear\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    data['time_idx'] = range(len(data))\n",
    "    \n",
    "    print(f\"Processed data shape: {data.shape}\")\n",
    "    print(f\"Date range: {data['TIMESTAMP'].min()} to {data['TIMESTAMP'].max()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_tft_dataset(data, config_key):\n",
    "    \"\"\"Create TimeSeriesDataSet for TFT training\"\"\"\n",
    "    config = PREDICTION_CONFIGS[config_key]\n",
    "    \n",
    "    # Define feature categories for TFT (only basic features)\n",
    "    time_varying_known_reals = [\n",
    "        'hour', 'day_of_week', 'month', 'day_of_year',\n",
    "        'U10', 'V10', 'U100', 'V100'\n",
    "    ]\n",
    "    \n",
    "    time_varying_unknown_reals = ['TARGETVAR']\n",
    "    \n",
    "    print(f\"Known reals: {time_varying_known_reals}\")\n",
    "    print(f\"Unknown reals: {time_varying_unknown_reals}\")\n",
    "    \n",
    "    # Determine training cutoff (70% for training)\n",
    "    max_prediction_length = config['max_prediction_length']\n",
    "    max_encoder_length = config['max_encoder_length']\n",
    "    training_cutoff = int(len(data) * 0.7)\n",
    "    \n",
    "    print(f\"Training cutoff: {training_cutoff} (out of {len(data)})\")\n",
    "    \n",
    "    # Create training dataset\n",
    "    training_data = TimeSeriesDataSet(\n",
    "        data=data[lambda x: x.time_idx <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"TARGETVAR\",\n",
    "        group_ids=[\"series_id\"],\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=[\"series_id\"], \n",
    "            transformation=\"softplus\",\n",
    "            center=True\n",
    "        ),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "    \n",
    "    # Create validation dataset\n",
    "    validation_data = TimeSeriesDataSet.from_dataset(\n",
    "        training_data,\n",
    "        data[lambda x: x.time_idx > training_cutoff],\n",
    "        predict=True,\n",
    "        stop_randomization=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(training_data)}\")\n",
    "    print(f\"Validation samples: {len(validation_data)}\")\n",
    "    \n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d6cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tft_model(training_data, validation_data, config_key):\n",
    "    \"\"\"Train Temporal Fusion Transformer model\"\"\"\n",
    "    config = PREDICTION_CONFIGS[config_key]\n",
    "    \n",
    "    print(f\"\\nTraining TFT for {config['description']}...\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = training_data.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'], \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_dataloader = validation_data.to_dataloader(\n",
    "        train=False, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'] * 2, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Find optimal learning rate (optional)\n",
    "    print(\"Finding optimal learning rate...\")\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Create TFT model\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training_data,\n",
    "        learning_rate=TFT_PARAMS['learning_rate'],\n",
    "        hidden_size=TFT_PARAMS['hidden_size'],\n",
    "        attention_head_size=TFT_PARAMS['attention_head_size'],\n",
    "        dropout=TFT_PARAMS['dropout'],\n",
    "        hidden_continuous_size=TFT_PARAMS['hidden_continuous_size'],\n",
    "        output_size=TFT_PARAMS['output_size'],\n",
    "        loss=QuantileLoss(quantiles=QUANTILES),\n",
    "        log_interval=TFT_PARAMS['log_interval'],\n",
    "        reduce_on_plateau_patience=TFT_PARAMS['reduce_on_plateau_patience'],\n",
    "        optimizer=TFT_PARAMS['optimizer'],\n",
    "    )\n",
    "    \n",
    "    print(f\"Model created with {tft.size()/1e3:.1f}k parameters\")\n",
    "    \n",
    "    # Configure trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=TRAINING_PARAMS['max_epochs'],\n",
    "        accelerator=\"auto\",\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=TRAINING_PARAMS['gradient_clip_val'],\n",
    "        limit_train_batches=50,  # Speed up training for demo\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(logging_interval=\"step\"),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\", \n",
    "                min_delta=1e-4, \n",
    "                patience=TRAINING_PARAMS['patience'], \n",
    "                verbose=False, \n",
    "                mode=\"min\"\n",
    "            ),\n",
    "        ],\n",
    "        \n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "    \n",
    "    return tft, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7781ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the cell that creates the input tensor\n",
    "# Replace the entire cell with:\n",
    "\n",
    "# Create a simple input example for visualization\n",
    "input_features = ['hour', 'day_of_week', 'month', 'day_of_year', 'U10', 'V10', 'U100', 'V100']\n",
    "input_values = [0] * len(input_features)  # Placeholder values\n",
    "input_tensor = torch.tensor([input_values], dtype=torch.float32).unsqueeze(0)  # shape: (1, 1, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7986022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wind power data...\n",
      "Raw data shape: (16800, 6)\n",
      "Processed data shape: (16765, 12)\n",
      "Date range: 2012-01-01 01:00:00 to 2013-11-30 00:00:00\n",
      "Known reals: ['hour', 'day_of_week', 'month', 'day_of_year', 'U10', 'V10', 'U100', 'V100']\n",
      "Unknown reals: ['TARGETVAR']\n",
      "Training cutoff: 11735 (out of 16765)\n",
      "Training samples: 11747\n",
      "Validation samples: 1\n",
      "TFT Model Summary:\n",
      "==================================================\n",
      "Number of parameters: 22,079\n",
      "Model components:\n",
      "  - loss: QuantileLoss\n",
      "  - logging_metrics: ModuleList\n",
      "  - input_embeddings: MultiEmbedding\n",
      "  - prescalers: ModuleDict\n",
      "  - static_variable_selection: VariableSelectionNetwork\n",
      "  - encoder_variable_selection: VariableSelectionNetwork\n",
      "  - decoder_variable_selection: VariableSelectionNetwork\n",
      "  - static_context_variable_selection: GatedResidualNetwork\n",
      "  - static_context_initial_hidden_lstm: GatedResidualNetwork\n",
      "  - static_context_initial_cell_lstm: GatedResidualNetwork\n",
      "  - static_context_enrichment: GatedResidualNetwork\n",
      "  - lstm_encoder: LSTM\n",
      "  - lstm_decoder: LSTM\n",
      "  - post_lstm_gate_encoder: GatedLinearUnit\n",
      "  - post_lstm_add_norm_encoder: AddNorm\n",
      "  - static_enrichment: GatedResidualNetwork\n",
      "  - multihead_attn: InterpretableMultiHeadAttention\n",
      "  - post_attn_gate_norm: GateAddNorm\n",
      "  - pos_wise_ff: GatedResidualNetwork\n",
      "  - pre_output_gate_norm: GateAddNorm\n",
      "  - output_layer: Linear\n",
      "\n",
      "Input shapes:\n",
      "  encoder_cat: torch.Size([1, 168, 0])\n",
      "  encoder_cont: torch.Size([1, 168, 13])\n",
      "  encoder_target: torch.Size([1, 168])\n",
      "  encoder_lengths: torch.Size([1])\n",
      "  decoder_cat: torch.Size([1, 12, 0])\n",
      "  decoder_cont: torch.Size([1, 12, 13])\n",
      "  decoder_target: torch.Size([1, 12])\n",
      "  decoder_lengths: torch.Size([1])\n",
      "  decoder_time_idx: torch.Size([1, 12])\n",
      "  groups: torch.Size([1, 1])\n",
      "  target_scale: torch.Size([1, 2])\n",
      "\n",
      "Model Architecture Diagram:\n",
      "==================================================\n",
      "Input → Embeddings → Variable Selection → LSTM Encoder/Decoder\n",
      "→ Static Context → Attention → Output Layer\n",
      "Visualizing TFT Model Flow...\n",
      "TFT Model Flow Visualization\n",
      "==================================================\n",
      "input-tensor depth:0\n",
      "encoder_cat: (1, 168, 0)\n",
      "encoder_cont: (1, 168, 13)\n",
      "encoder_target: (1, 168)\n",
      "encoder_lengths: (1,)\n",
      "decoder_cat: (1, 12, 0)\n",
      "decoder_cont: (1, 12, 13)\n",
      "decoder_target: (1, 12)\n",
      "decoder_lengths: (1,)\n",
      "decoder_time_idx: (1, 12)\n",
      "groups: (1, 1)\n",
      "target_scale: (1, 2)\n",
      "\n",
      "InputEmbeddings\n",
      "input: Various input features\n",
      "depth:1\n",
      "output: Embedded representations\n",
      "\n",
      "VariableSelectionNetwork (Static)\n",
      "input: Static embeddings\n",
      "depth:1\n",
      "output: Selected static features\n",
      "\n",
      "VariableSelectionNetwork (Encoder)\n",
      "input: Time-varying known inputs\n",
      "depth:1\n",
      "output: Selected encoder features\n",
      "\n",
      "VariableSelectionNetwork (Decoder)\n",
      "input: Time-varying known inputs (future)\n",
      "depth:1\n",
      "output: Selected decoder features\n",
      "\n",
      "LSTM Encoder\n",
      "input: Selected encoder features\n",
      "depth:1\n",
      "output: Encoder hidden states\n",
      "\n",
      "LSTM Decoder\n",
      "input: Selected decoder features + static context\n",
      "depth:1\n",
      "output: Decoder hidden states\n",
      "\n",
      "InterpretableMultiHeadAttention\n",
      "input: Encoder states + Decoder states\n",
      "depth:1\n",
      "output: Attention-weighted representations\n",
      "\n",
      "GatedResidualNetwork\n",
      "input: Attention output + static context\n",
      "depth:1\n",
      "output: Enriched representations\n",
      "\n",
      "OutputLayer\n",
      "input: Final representations\n",
      "depth:1\n",
      "output: Quantile predictions\n",
      "\n",
      "output-tensor depth:0\n",
      "output: (1, 12, 7)\n"
     ]
    }
   ],
   "source": [
    "# Replace the problematic cell with this code:\n",
    "\n",
    "# Alternative approach to visualize the TFT model structure\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load and process the data first\n",
    "FILE_PATH = r\"C:\\Users\\User\\Documents\\Semester 07\\EE4750 - Data Analytics in Power Systems\\WindPowerForecastingData TASK.xlsx\\WindPowerForecastingData TASK.xlsx\"\n",
    "processed_data = load_wind_data(FILE_PATH)\n",
    "\n",
    "# Create a proper input sample from the dataset\n",
    "config = PREDICTION_CONFIGS['12h']\n",
    "training_data, _ = create_tft_dataset(processed_data, '12h')\n",
    "\n",
    "# Create a simple TFT model\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    training_data,\n",
    "    learning_rate=TFT_PARAMS['learning_rate'],\n",
    "    hidden_size=16,  # Smaller for visualization\n",
    "    attention_head_size=2,\n",
    "    dropout=TFT_PARAMS['dropout'],\n",
    "    hidden_continuous_size=4,\n",
    "    output_size=TFT_PARAMS['output_size'],\n",
    "    loss=QuantileLoss(quantiles=QUANTILES),\n",
    ")\n",
    "\n",
    "# Print model summary instead of using torchview\n",
    "print(\"TFT Model Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get a sample batch to understand input dimensions\n",
    "sample_batch = next(iter(training_data.to_dataloader(train=True, batch_size=1)))\n",
    "input_dict = sample_batch[0]  # Extract the input dictionary\n",
    "\n",
    "# Print information about the model structure\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in tft_model.parameters()):,}\")\n",
    "print(f\"Model components:\")\n",
    "for name, module in tft_model.named_children():\n",
    "    print(f\"  - {name}: {module.__class__.__name__}\")\n",
    "    \n",
    "# Print input shapes\n",
    "print(\"\\nInput shapes:\")\n",
    "for key, value in input_dict.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "\n",
    "# Create a simple diagram of the model architecture\n",
    "print(\"\\nModel Architecture Diagram:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Input → Embeddings → Variable Selection → LSTM Encoder/Decoder\")\n",
    "print(\"→ Static Context → Attention → Output Layer\")\n",
    "def visualize_tft_flow(model, sample_batch):\n",
    "    \"\"\"\n",
    "    Create a text-based flow visualization similar to the provided image\n",
    "    but for the Temporal Fusion Transformer model\n",
    "    \"\"\"\n",
    "    # Extract the input dictionary from the sample batch\n",
    "    input_dict = sample_batch[0]\n",
    "    \n",
    "    print(\"TFT Model Flow Visualization\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Print input information\n",
    "    print(\"input-tensor depth:0\")\n",
    "    for key, value in input_dict.items():\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"{key}: {tuple(value.shape)}\")\n",
    "    print()\n",
    "    \n",
    "    # Manually trace through the main components of TFT\n",
    "    print(\"InputEmbeddings\")\n",
    "    print(\"input: Various input features\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Embedded representations\")\n",
    "    print()\n",
    "    \n",
    "    print(\"VariableSelectionNetwork (Static)\")\n",
    "    print(\"input: Static embeddings\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Selected static features\")\n",
    "    print()\n",
    "    \n",
    "    print(\"VariableSelectionNetwork (Encoder)\")\n",
    "    print(\"input: Time-varying known inputs\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Selected encoder features\")\n",
    "    print()\n",
    "    \n",
    "    print(\"VariableSelectionNetwork (Decoder)\")\n",
    "    print(\"input: Time-varying known inputs (future)\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Selected decoder features\")\n",
    "    print()\n",
    "    \n",
    "    print(\"LSTM Encoder\")\n",
    "    print(\"input: Selected encoder features\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Encoder hidden states\")\n",
    "    print()\n",
    "    \n",
    "    print(\"LSTM Decoder\")\n",
    "    print(\"input: Selected decoder features + static context\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Decoder hidden states\")\n",
    "    print()\n",
    "    \n",
    "    print(\"InterpretableMultiHeadAttention\")\n",
    "    print(\"input: Encoder states + Decoder states\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Attention-weighted representations\")\n",
    "    print()\n",
    "    \n",
    "    print(\"GatedResidualNetwork\")\n",
    "    print(\"input: Attention output + static context\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Enriched representations\")\n",
    "    print()\n",
    "    \n",
    "    print(\"OutputLayer\")\n",
    "    print(\"input: Final representations\")\n",
    "    print(\"depth:1\")\n",
    "    print(\"output: Quantile predictions\")\n",
    "    print()\n",
    "    \n",
    "    # Get the output shape\n",
    "    with torch.no_grad():\n",
    "        output = model(input_dict)\n",
    "        output_shape = output[0].shape if isinstance(output, tuple) else output.shape\n",
    "    \n",
    "    print(\"output-tensor depth:0\")\n",
    "    print(f\"output: {tuple(output_shape)}\")\n",
    "\n",
    "# Use the function to visualize the TFT flow\n",
    "print(\"Visualizing TFT Model Flow...\")\n",
    "visualize_tft_flow(tft_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc2b0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, validation_data, trainer, config_key):\n",
    "    \"\"\"Generate predictions using trained TFT model\"\"\"\n",
    "    config = PREDICTION_CONFIGS[config_key]\n",
    "    \n",
    "    print(f\"Making predictions for {config['description']}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    val_dataloader = validation_data.to_dataloader(\n",
    "        train=False, batch_size=TRAINING_PARAMS['batch_size'] * 4, num_workers=0\n",
    "    )\n",
    "    \n",
    "    predictions = trainer.predict(model, dataloaders=val_dataloader)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_tft_predictions(predictions, config_key):\n",
    "    \"\"\"Evaluate TFT predictions\"\"\"\n",
    "    config = PREDICTION_CONFIGS[config_key]\n",
    "    \n",
    "    # Extract predictions and actuals\n",
    "    prediction_values = torch.cat([p[0] for p in predictions], dim=0)\n",
    "    prediction_index = pd.concat([p[1] for p in predictions], ignore_index=True)\n",
    "    \n",
    "    # Get median predictions (quantile 0.5)\n",
    "    median_idx = len(QUANTILES) // 2\n",
    "    median_predictions = prediction_values[:, :, median_idx].numpy()\n",
    "    \n",
    "    # Calculate metrics (simplified - you may want to get actual values from your data)\n",
    "    print(f\"\\n{config['description']} TFT Results:\")\n",
    "    print(f\"Prediction shape: {median_predictions.shape}\")\n",
    "    print(f\"Number of forecasts: {len(median_predictions)}\")\n",
    "    print(f\"Forecast horizon: {config['max_prediction_length']} hours\")\n",
    "    \n",
    "    return median_predictions, prediction_index\n",
    "\n",
    "def plot_tft_predictions(predictions, prediction_index, config_key, num_samples=3):\n",
    "    \"\"\"Plot TFT prediction results\"\"\"\n",
    "    config = PREDICTION_CONFIGS[config_key]\n",
    "    \n",
    "    prediction_values = torch.cat([p[0] for p in predictions], dim=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(15, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(min(num_samples, len(prediction_values))):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get prediction quantiles\n",
    "        pred_sample = prediction_values[i].numpy()  # Shape: (forecast_length, num_quantiles)\n",
    "        time_steps = range(pred_sample.shape[0])\n",
    "        \n",
    "        # Plot median prediction\n",
    "        median_idx = len(QUANTILES) // 2\n",
    "        ax.plot(time_steps, pred_sample[:, median_idx], 'r-', \n",
    "                label='Median Prediction', linewidth=2)\n",
    "        \n",
    "        # Plot confidence intervals\n",
    "        lower_idx = 1  # 10% quantile\n",
    "        upper_idx = -2  # 90% quantile\n",
    "        ax.fill_between(time_steps, \n",
    "                       pred_sample[:, lower_idx], \n",
    "                       pred_sample[:, upper_idx], \n",
    "                       alpha=0.3, color='red', \n",
    "                       label='80% Prediction Interval')\n",
    "        \n",
    "        # Plot 95% confidence interval\n",
    "        ax.fill_between(time_steps, \n",
    "                       pred_sample[:, 0], \n",
    "                       pred_sample[:, -1], \n",
    "                       alpha=0.2, color='orange', \n",
    "                       label='95% Prediction Interval')\n",
    "        \n",
    "        ax.set_title(f'TFT {config[\"description\"]} - Sample {i+1}')\n",
    "        ax.set_xlabel('Hours ahead')\n",
    "        ax.set_ylabel('Wind Power')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, training_data):\n",
    "    \"\"\"Plot TFT feature importance\"\"\"\n",
    "    # Get interpretation\n",
    "    interpretation = model.interpret_output(\n",
    "        training_data.to_dataloader(train=False, batch_size=1000),\n",
    "        reduce_on_plateau_patience=4,\n",
    "        return_attention=True\n",
    "    )\n",
    "    \n",
    "    # Plot variable importance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Encoder variable importance\n",
    "    model.plot_interpretation(interpretation, axes=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Variable Importance\")\n",
    "    \n",
    "    # Attention patterns\n",
    "    model.plot_interpretation(interpretation, axes=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Attention Patterns\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f9ee338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tft_wind_forecasting(file_path, horizons_to_test=['12h']):\n",
    "    \"\"\"Run complete TFT wind power forecasting pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting TFT Wind Power Forecasting Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_wind_data(file_path)\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    for horizon in horizons_to_test:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing {PREDICTION_CONFIGS[horizon]['description']} forecast with TFT\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Create TFT dataset\n",
    "            training_data, validation_data = create_tft_dataset(data, horizon)\n",
    "            \n",
    "            # Train TFT model\n",
    "            tft_model, trainer = train_tft_model(training_data, validation_data, horizon)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = make_predictions(tft_model, validation_data, trainer, horizon)\n",
    "            \n",
    "            # Evaluate predictions\n",
    "            pred_values, pred_index = evaluate_tft_predictions(predictions, horizon)\n",
    "            \n",
    "            # Plot results\n",
    "            plot_tft_predictions(predictions, pred_index, horizon)\n",
    "            \n",
    "            # Plot feature importance (for the first model only)\n",
    "            if horizon == horizons_to_test[0]:\n",
    "                print(\"Plotting feature importance...\")\n",
    "                try:\n",
    "                    plot_feature_importance(tft_model, training_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not plot feature importance: {e}\")\n",
    "            \n",
    "            # Store results\n",
    "            results[horizon] = {\n",
    "                'predictions': pred_values,\n",
    "                'index': pred_index,\n",
    "                'model_size': tft_model.size()\n",
    "            }\n",
    "            models[horizon] = tft_model\n",
    "            \n",
    "            print(f\"✓ {horizon} forecast completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {horizon} forecast: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TFT WIND FORECASTING RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for horizon in results:\n",
    "        config = PREDICTION_CONFIGS[horizon]\n",
    "        result = results[horizon]\n",
    "        print(f\"\\n{config['description']}:\")\n",
    "        print(f\"  Model parameters: {result['model_size']:,}\")\n",
    "        print(f\"  Prediction horizon: {config['max_prediction_length']} hours\")\n",
    "        print(f\"  Number of forecasts: {len(result['predictions'])}\")\n",
    "    \n",
    "    return results, models, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7851731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from tst import PREDICTION_HORIZONS, load_and_prepare_data\n",
    "\n",
    "\n",
    "def run_forecasting(file_path, horizons_to_test=['12h']):\n",
    "    \"\"\"Run complete forecasting pipeline\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    features, timestamps = load_and_prepare_data(file_path)\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    for horizon_name in horizons_to_test:\n",
    "        horizon_hours = PREDICTION_HORIZONS[horizon_name]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training model for {horizon_hours}-hour forecast\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        train_loader, val_loader, test_loader, scaler = prepare_datasets(features, horizon_hours)\n",
    "        \n",
    "        # Create model\n",
    "        model = WindPowerLSTM(\n",
    "            input_size=features.shape[1],  # All features\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            forecast_horizon=horizon_hours,\n",
    "            dropout=DROPOUT\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Train model\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, NUM_EPOCHS)\n",
    "        \n",
    "        # Evaluate model\n",
    "        predictions, actuals, metrics = evaluate_model(model, test_loader, scaler, f\"{horizon_hours}h\")\n",
    "        \n",
    "        # Plot results\n",
    "        plot_predictions(predictions, actuals, f\"{horizon_hours}h\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title(f'Training Progress - {horizon_hours}h Forecast')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Store results\n",
    "        results[horizon_name] = metrics\n",
    "        models[horizon_name] = model\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FORECASTING RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for horizon_name, metrics in results.items():\n",
    "        hours = PREDICTION_HORIZONS[horizon_name]\n",
    "        print(f\"\\n{hours}-hour forecast:\")\n",
    "        print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "    \n",
    "    return results, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba517b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TFT Wind Power Forecasting...\n",
      "Starting TFT Wind Power Forecasting Pipeline\n",
      "============================================================\n",
      "Loading wind power data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (16800, 6)\n",
      "Processed data shape: (16765, 12)\n",
      "Date range: 2012-01-01 01:00:00 to 2013-11-30 00:00:00\n",
      "\n",
      "================================================================================\n",
      "Processing 12 hours ahead forecast with TFT\n",
      "================================================================================\n",
      "Known reals: ['hour', 'day_of_week', 'month', 'day_of_year', 'U10', 'V10', 'U100', 'V100']\n",
      "Unknown reals: ['TARGETVAR']\n",
      "Training cutoff: 11735 (out of 16765)\n",
      "Training samples: 11747\n",
      "Validation samples: 1\n",
      "\n",
      "Training TFT for 12 hours ahead...\n",
      "Finding optimal learning rate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 246.7k parameters\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 208    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 18.2 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 16.2 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 455    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "246 K     Trainable params\n",
      "0         Non-trainable params\n",
      "246 K     Total params\n",
      "0.987     Total estimated model params size (MB)\n",
      "482       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  78%|███████▊  | 39/50 [07:27<02:06,  0.09it/s, v_num=9, train_loss_step=0.276]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set your file path\n",
    "    FILE_PATH = r\"C:\\Users\\User\\Documents\\Semester 07\\EE4750 - Data Analytics in Power Systems\\WindPowerForecastingData TASK.xlsx\\WindPowerForecastingData TASK.xlsx\"\n",
    "    \n",
    "    # Choose horizons to test\n",
    "    HORIZONS_TO_TEST = ['12h']  # Start with shorter horizons\n",
    "    \n",
    "    print(\"Running TFT Wind Power Forecasting...\")\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    tft_results, tft_models, processed_data = run_tft_wind_forecasting(\n",
    "        FILE_PATH, \n",
    "        HORIZONS_TO_TEST\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTFT Pipeline completed!\")\n",
    "    print(f\"Trained models for: {list(tft_results.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind-tft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
