{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54293e0",
   "metadata": {},
   "source": [
    "# 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "818bf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_forecasting as pf\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import (ConcatDataset, DataLoader, Dataset, Subset,\n",
    "                              random_split)\n",
    "from torchvision import datasets, transforms\n",
    "import seaborn as sns\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1ea3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------------+---------+-----------+---------+-----------+\n",
      "|    | TIMESTAMP     |   TARGETVAR |     U10 |       V10 |    U100 |      V100 |\n",
      "|----+---------------+-------------+---------+-----------+---------+-----------|\n",
      "|  0 | 20120101 1:00 |   0         | 2.1246  | -2.68197  | 2.86428 | -3.66608  |\n",
      "|  1 | 20120101 2:00 |   0.0548791 | 2.52169 | -1.79696  | 3.34486 | -2.46476  |\n",
      "|  2 | 20120101 3:00 |   0.110234  | 2.67221 | -0.822516 | 3.50845 | -1.21409  |\n",
      "|  3 | 20120101 4:00 |   0.165116  | 2.4575  | -0.143642 | 3.21523 | -0.355546 |\n",
      "|  4 | 20120101 5:00 |   0.15694   | 2.2459  |  0.389576 | 2.95768 |  0.332701 |\n",
      "+----+---------------+-------------+---------+-----------+---------+-----------+\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\User\\Documents\\Semester 07\\EE4750 - Data Analytics in Power Systems\\WindPowerForecastingData TASK.xlsx\\WindPowerForecastingData TASK.xlsx\"\n",
    "data = pd.read_excel(path, engine='openpyxl')\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "61e976aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+---------+-----------+---------+-----------+------+\n",
      "| TIMESTAMP           |   TARGETVAR |     U10 |       V10 |    U100 |      V100 |   ID |\n",
      "|---------------------+-------------+---------+-----------+---------+-----------+------|\n",
      "| 2012-01-01 01:00:00 |   0         | 2.1246  | -2.68197  | 2.86428 | -3.66608  |    1 |\n",
      "| 2012-01-01 02:00:00 |   0.0548791 | 2.52169 | -1.79696  | 3.34486 | -2.46476  |    2 |\n",
      "| 2012-01-01 03:00:00 |   0.110234  | 2.67221 | -0.822516 | 3.50845 | -1.21409  |    3 |\n",
      "| 2012-01-01 04:00:00 |   0.165116  | 2.4575  | -0.143642 | 3.21523 | -0.355546 |    4 |\n",
      "| 2012-01-01 05:00:00 |   0.15694   | 2.2459  |  0.389576 | 2.95768 |  0.332701 |    5 |\n",
      "+---------------------+-------------+---------+-----------+---------+-----------+------+\n"
     ]
    }
   ],
   "source": [
    "data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'])\n",
    "data = data.set_index('TIMESTAMP')\n",
    "# Add incremental ID\n",
    "data['ID'] = range(1, len(data)+1)\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ee8089c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "def fit_preprocessing(train, real_columns, categorical_columns):\n",
    "    real_scalers = StandardScaler().fit(train[real_columns].values)\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "    for col in categorical_columns:\n",
    "        srs = train[col].apply(str) \n",
    "        categorical_scalers[col] = LabelEncoder().fit(srs.values)\n",
    "        num_classes.append(srs.nunique())\n",
    "\n",
    "    return real_scalers, categorical_scalers\n",
    "\n",
    "\n",
    "def transform_inputs(df, real_scalers, categorical_scalers, real_columns, categorical_columns):\n",
    "    out = df.copy()\n",
    "    out[real_columns] = real_scalers.transform(df[real_columns].values)\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        string_df = df[col].apply(str)\n",
    "        out[col] = categorical_scalers[col].transform(string_df)\n",
    "\n",
    "    return out\n",
    "\n",
    "real_columns = ['TARGETVAR', 'U10', 'V10', 'U100', 'V100']\n",
    "categorical_columns = []   # add any categorical column names if you have them\n",
    "\n",
    "# Fit on the training data\n",
    "real_scalers, categorical_scalers = fit_preprocessing(\n",
    "    data, real_columns, categorical_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7fe2ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\wind-tft\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess as DataFrames\n",
    "real_scalers, categorical_scalers = fit_preprocessing(\n",
    "    data, real_columns, categorical_columns\n",
    ")\n",
    "scaled_df = transform_inputs(data, real_scalers, categorical_scalers,\n",
    "                             real_columns, categorical_columns)\n",
    "\n",
    "# 2. Split the *scaled* DataFrame\n",
    "train_df, val_df, test_df = np.split(\n",
    "    scaled_df.sample(frac=1, random_state=42),\n",
    "    [int(.6*len(scaled_df)), int(.8*len(scaled_df))]\n",
    ")\n",
    "\n",
    "# 3. Convert to tensors\n",
    "train_dataset = TensorDataset(torch.tensor(train_df.values, dtype=torch.float32))\n",
    "val_dataset   = TensorDataset(torch.tensor(val_df.values,   dtype=torch.float32))\n",
    "test_dataset  = TensorDataset(torch.tensor(test_df.values,  dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "938b8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "ENCODER_STEPS = 175\n",
    "DECODER_STEPS = 180\n",
    "HIDDEN_LAYER_SIZE = 80\n",
    "EMBEDDING_DIMENSION = 8\n",
    "NUM_LSTM_LAYERS = 1\n",
    "NUM_ATTENTION_HEADS = 2\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc50eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables\n",
    "input_columns = ['U10', 'V10', 'U100', 'V100']\n",
    "target_column = 'TARGETVAR'\n",
    "time_column = 'TIMESTAMP'\n",
    "col_to_idx = {col: idx for idx, col in enumerate(input_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e241e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"quantiles\": QUANTILES,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"device\": DEVICE,\n",
    "    \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "    \"num_lstm_layers\": NUM_LSTM_LAYERS,\n",
    "    \"embedding_dim\": EMBEDDING_DIMENSION,\n",
    "    \"encoder_steps\": ENCODER_STEPS,\n",
    "    \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "    \"col_to_idx\": col_to_idx,\n",
    "    \"time_dependent_continuous\": input_columns,\n",
    "    \"known_time_dependent\": input_columns,\n",
    "    \"observed_time_dependent\": target_column\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a7ba6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TFT_Dataset(Dataset):\n",
    "    def __init__(self, data, entity_column, time_column, target_column, \n",
    "                 input_columns, encoder_steps, decoder_steps):\n",
    "        \"\"\"\n",
    "        data (pd.DataFrame): dataframe containing raw data\n",
    "        entity_column (str): name of column containing entity data\n",
    "        time_column (str): name of column containing date data\n",
    "        target_column (str): name of column we need to predict\n",
    "        input_columns (list): list of string names of columns used as input\n",
    "        encoder_steps (int): number of known past time steps used for forecast\n",
    "        decoder_steps (int): number of input time steps used for each forecast date\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder_steps = encoder_steps\n",
    "        self.decoder_steps = decoder_steps\n",
    "        inputs, outputs, entity, time = [], [], [], []\n",
    "\n",
    "        for e_val in data[entity_column].unique():\n",
    "            entity_group = data[data[entity_column] == e_val]\n",
    "            data_time_steps = len(entity_group)\n",
    "\n",
    "            if data_time_steps >= decoder_steps:\n",
    "                x = entity_group[input_columns].values.astype(np.float32)\n",
    "                inputs.append(\n",
    "                    np.stack([x[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1)\n",
    "                )\n",
    "\n",
    "                y = entity_group[[target_column]].values.astype(np.float32)\n",
    "                outputs.append(\n",
    "                    np.stack([y[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1)\n",
    "                )\n",
    "\n",
    "                e_arr = entity_group[[entity_column]].values.astype(np.float32)\n",
    "                entity.append(\n",
    "                    np.stack([e_arr[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1)\n",
    "                )\n",
    "\n",
    "                t = entity_group[[time_column]].values.astype(np.int64)\n",
    "                time.append(\n",
    "                    np.stack([t[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1)\n",
    "                )\n",
    "\n",
    "        # Concatenate all entities\n",
    "        self.inputs = np.concatenate(inputs, axis=0)\n",
    "        self.outputs = np.concatenate(outputs, axis=0)[:, encoder_steps:, :]\n",
    "        self.entity = np.concatenate(entity, axis=0)\n",
    "        self.time = np.concatenate(time, axis=0)\n",
    "        self.active_inputs = np.ones_like(self.outputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'inputs': self.inputs[index],\n",
    "            'outputs': self.outputs[index],\n",
    "            'active_entries': self.active_inputs[index],\n",
    "            'time': self.time[index],\n",
    "            'identifier': self.entity[index]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e5485518",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TIMESTAMP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\wind-tft\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TIMESTAMP'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTIMESTAMP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\wind-tft\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\wind-tft\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TIMESTAMP'"
     ]
    }
   ],
   "source": [
    "data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'])\n",
    "data[\"ID\"] = 0\n",
    "data = data.set_index('TIMESTAMP')\n",
    "\n",
    "training_data = TFT_Dataset(train_df.reset_index(), \n",
    "                            entity_column=\"ID\",   # <--- add this\n",
    "                            time_column=time_column, \n",
    "                            target_column=target_column, \n",
    "                            input_columns=input_columns, \n",
    "                            encoder_steps=ENCODER_STEPS, \n",
    "                            decoder_steps=DECODER_STEPS)\n",
    "\n",
    "validation_data = TFT_Dataset(val_df.reset_index(), \n",
    "                              entity_column=\"ID\", \n",
    "                              time_column=time_column, \n",
    "                              target_column=target_column, \n",
    "                              input_columns=input_columns, \n",
    "                              encoder_steps=ENCODER_STEPS, \n",
    "                              decoder_steps=DECODER_STEPS)\n",
    "\n",
    "testing_data = TFT_Dataset(test_df.reset_index(), \n",
    "                           entity_column=\"ID\", \n",
    "                           time_column=time_column, \n",
    "                           target_column=target_column, \n",
    "                           input_columns=input_columns, \n",
    "                           encoder_steps=ENCODER_STEPS, \n",
    "                           decoder_steps=DECODER_STEPS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind-tft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
